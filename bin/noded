#!/usr/bin/env python

from __future__ import print_function, division#, unicode_literals

import argparse
import json
import os
import sys
import time
from collections import defaultdict, deque
from glob import glob
from pwd import getpwuid
from socket import gethostname

try:
    from ConfigParser import ConfigParser
except ImportError:
    from configparser import ConfigParser

try:
    import redis
except ImportError:
    print("You must install python-redis.")
    sys.exit(1)


def parse_config(config):
    """Parse noded.conf and return a ConfigParser object."""
    if os.path.isfile(config):
        noded_config = ConfigParser({"redis_auth": None})
        noded_config.read(config)
        return noded_config
    else:
        print("You need to specify path to noded.conf")
        sys.exit(1)


def expand_cpuset(cpusetcpus):
    """Return a list of CPUs, translated from a cgroup cpuset.

    For example, with a cpuset of "0-3", this function should return
    [0, 1, 2, 3].

    Args:
        cpusetcpus (str): A cpuset string from cpuset.cpus cgroup file
    Return:
        list: List of CPUs included in a provided cpuset
    """
    cpulist = []
    for cpus in cpusetcpus.split(","):
        if "-" in cpus:
            cpusplit = cpus.split("-")
            for cpu in range(int(cpusplit[0]), int(cpusplit[1]) + 1):
                cpulist.append(cpu)
        else:
            cpulist.append(int(cpus))
    return cpulist


def get_sys_info():
    """
    Return dictionary of system information

    SC_NPROCESSORS_ONLN returns the number of processors which are currently
    online (i.e. available).  See os.sysconf_names for dictionary values.

    """
    return {
        "nodename": gethostname(),
        "total_logical_cpus": os.sysconf(84)
    }


def find(name, path):
    result = []
    for root, dirs, files in os.walk(path):
        if name in files:
            result.append(os.path.join(root, name))
    return result


def get_running_threads(pid, procpath="/proc"):
    """Return a dict of running thread ids for a given process id."""

    # Search the pids task directory for a list of thread ids
    try:
        thread_ids = os.listdir(os.path.join(procpath, "%s/task") % pid)
    except OSError:
        # Short-lived thread disappeared on us; skip for now and try again
        # in the next round
        return

    # Sort the list
    thread_ids.sort()

    # Initialize running_threads with empty lists for each allocated cpu
    running_threads = defaultdict(list)

    # For each thread id in the list of all thread ids, append the thread id
    # to the 'list_of_threads' dictionary only if the thread is in the RUNNING
    # state
    for thread_id in thread_ids:
        pidstat = os.path.join(procpath, "%s/task/%s/stat") % (pid, thread_id)
        try:
            with open(pidstat, 'r') as fname:
                stat = fname.read().strip()
        except IOError:
            # An exception here means the thread disappeared on us;
            # just continue on with the next thread in this for loop
            continue
        else:
            # If threads didn't die on us, get some info for each thread
            if stat:
                values = stat.split()
                name = values[1].translate(None, "()")
                state = values[2]
                processor = values[38]

            # Only append thread to the list_of_threads if thread is in RUNNING
            # state
            if state == 'R' or state == 'D':
                running_threads[processor].append([name, state, thread_id])

    # Return dict of running threads
    return running_threads


def lineup_children(cpu_affinity, children_pids, procpath="/proc"):
    """Return the number of running threads and a cpu mapping for its processes."""

    num_running_children = 0

    # Create a dictionary of empty lists with keys corresponding to the
    # cpu_affinity
    cpu_and_procs = dict((cpu, []) for cpu in cpu_affinity)

    # For each child pid of a slurmstepd process, get all threads and
    # append them as a dictionary to a new list
    cpu_and_procs_list = []

    for child in children_pids:
        run_threads = get_running_threads(child, procpath)
        if run_threads:
            cpu_and_procs_list.append(run_threads)

    # For each thread in the above list, populate the precreated
    # cpu_and_procs dictionary
    for cpu_ddict in cpu_and_procs_list:
        try:
            for cpu, threads in cpu_ddict.iteritems():
                for threadinfo in threads:
                    cpu_and_procs[int(cpu)].append([threadinfo[0], threadinfo[1]])
        except AttributeError:
            continue

    num_running_children = sum(len(thread)
                               for thread in cpu_and_procs.itervalues())

    if num_running_children > len(cpu_affinity):
        job_overloaded = bool(1)
    else:
        job_overloaded = bool(0)

    return (num_running_children, cpu_and_procs, job_overloaded)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("-c", help="configuration file",
                        dest="config", default="conf/noded.conf")
    args = parser.parse_args()

    noded_config = parse_config(args.config)

    # Load up the system info once at runtime
    sysinfo = get_sys_info()

    # Redis constants
    redis_db = int(noded_config.get("defaults", "redis_db"))
    redis_port = int(noded_config.get("defaults", "redis_port"))
    redis_host = noded_config.get("defaults", "redis_host")
    redis_timeout = int(noded_config.get("defaults", "redis_timeout"))
    redis_auth = noded_config.get("defaults", "redis_auth")

    # Initialize connection to Redis
    redisconn = redis.StrictRedis(
        host=redis_host,
        port=redis_port,
        db=redis_db,
        password=redis_auth,
        socket_timeout=redis_timeout
    )

    # Set the sleep time (in seconds) between collecting metrics
    sleep_time = int(noded_config.get("defaults", "sleep_time"))

    # Set the expire time (in seconds) for Redis keys, in seconds
    expire_time = int(noded_config.get("defaults", "expire_time"))

    # Ring Buffer max length
    rb_maxlen = int(noded_config.get("defaults", "rb_maxlen"))

    # Get hostname
    host_name = gethostname()

    # Initialize ring buffer for determining overloaded node
    overloadq = deque([0, 0, 0, 0], rb_maxlen)

    # Main loop
    while 1:
        # Start by copying the sysinfo dictionary above...
        fullinfo = sysinfo.copy()

        meminfo = dict((m.split()[0].rstrip(':'), int(m.split()[1]))
                       for m in open('/proc/meminfo').readlines())

        memfree = meminfo["MemFree"]
        membuffers = meminfo["Buffers"]
        memcached = meminfo["Cached"]
        memtotal = meminfo["MemTotal"]
        memused = memtotal - (memfree + membuffers + memcached)
        swapfree = meminfo["SwapFree"]
        swaptotal = meminfo["SwapTotal"]
        swapused = swaptotal - swapfree

        # ... then append various metrics
        fullinfo.update({"load": os.getloadavg()[0]})
        fullinfo.update({"last_updated": time.time()})
        fullinfo.update({"total_memory": memtotal * 1024})
        fullinfo.update({"total_swap": swaptotal * 1024})
        fullinfo.update({"total_memory_used_percent": memused * 100 // memtotal})
        fullinfo.update({"total_swap_used_percent": swapused * 100 // swaptotal})

        # Initialize default
        jobs = defaultdict(list)
        node_overloaded = bool(0)

        # Get all jobids from the cgroup filesystem
        alljobs = [int(job.split("job_")[1])
                   for job in glob("/cgroup/cpuset/slurm/uid_*/job_*")]

        # If there any jobs, run it through the mill
        if alljobs:
            for jobid in alljobs:
                memlimitpath = glob("/cgroup/memory/slurm/uid_*/job_" +
                                         str(jobid) + "/memory.limit_in_bytes")

                memusagepath = glob("/cgroup/memory/slurm/uid_*/job_" +
                                         str(jobid) + "/memory.stat")

                cpusetcpuspath = glob("/cgroup/cpuset/slurm/uid_*/job_" +
                                           str(jobid) + "/cpuset.cpus")

                cpusetpath = glob("/cgroup/cpuset/slurm/uid_*/job_" +
                                       str(jobid))

                uidpath = glob("/cgroup/memory/slurm/uid_*/job_" +
                                    str(jobid))

                user = None
                if uidpath:
                    uid = os.path.split(uidpath[0])[0].split("uid_")[1]
                    user = getpwuid(int(uid))[0]

                memlimit = 0
                if memlimitpath:
                    with open(memlimitpath[0], "r") as fname:
                        memlimit = fname.read().strip()

                memusage = 0
                if memusagepath:
                    with open(memusagepath[0], "r") as fname:
                        for line in fname.readlines():
                            if line.startswith("total_rss"):
                                memusage += int(line.strip().split()[1])
                            if line.startswith("total_mapped_file"):
                                memusage += int(line.strip().split()[1])

                if cpusetcpuspath:
                    with open(cpusetcpuspath[0], "r") as fname:
                        cpusetcpus = expand_cpuset(fname.read().strip())
                else:
                    cpusetcpus = []

                proclist = set()

                if cpusetpath:
                    for path in find("cgroup.procs", cpusetpath[0]):
                        try:
                            with open(path, "r") as fname:
                                for proc in fname.readlines():
                                    proclist.add(proc.strip())
                        except IOError:
                            continue

                num_running_children, cpu_and_procs, job_overloaded = lineup_children(cpusetcpus, proclist)


                if job_overloaded:
                    node_overloaded = bool(1)

                jobs[jobid] = {"mem_alloc": memlimit,
                               "mem_usage": memusage,
                               "active_threads": num_running_children,
                               "cpu_and_procs": cpu_and_procs,
                               "overloaded": job_overloaded,
                               "user": user}

            fullinfo.update({"jobs": jobs})

        if node_overloaded:
            overloadq.append(1)
        else:
            overloadq.append(0)

        # If overloaded queue (overloadq) is full, node has an overloaded for
        # rb_maxlen * sleep seconds
        if sum(overloadq) == rb_maxlen:
            fullinfo.update({"overloaded": bool(1)})
        else:
            fullinfo.update({"overloaded": bool(0)})

        try:
            # Push fullinfo dictionary as JSON to Redis DB;
            # Then, sleep for 30 seconds
            redisconn.set(host_name, json.dumps(fullinfo), ex=expire_time)
            time.sleep(sleep_time)
        except (redis.exceptions.ConnectionError,
                redis.exceptions.ResponseError,
                redis.exceptions.TimeoutError):
            # If Redis DB is down, sleep for sleep_time seconds and try again
            time.sleep(sleep_time)
            continue
